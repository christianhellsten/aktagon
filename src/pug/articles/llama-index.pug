extends ../article-layout.pug

block title
  title= 'Index and Search Your Data Seamlessly with LlamaIndex and Large Language Models'

block content
  h1 Index and Search Your Data Seamlessly with LlamaIndex and Large Language Models

  :markdown-it(linkify langPrefix='highlight-' plugins=['markdown-it-table-of-contents', 'markdown-it-anchor'])

    **Table of contents**

    [[toc]]

    ## Introduction

    Large language models (LLMs) like ChatGPT have a human-like ability to
    understand languages, enabling them to respond to questions and prompts in
    a manner that resembles a human conversation.

    Despite this impressive ability, there is always a risk that an LLM will
    fail to provide a relevant and accurate answer. This can be attributed to
    various factors, primarily the limited, and sometimes the outdated, nature
    of the data on which the LLM was trained on, but also the stochastic nature
    of the algorithms they employ.

    As an example, ChatGPT will not be able to provide relevant answers when
    the question concerns private data that the language model has not been
    trained on.

    In such cases, LLMs can be fine-tuned using the private data to achieve
    more accurate responses. However, fine-tuning can be a resource intensive
    and costly operation, and has to be done again whenever the data is updated.

    Often a more effective and cost-efficient approach involves prompt
    engineering and local pre-processing and retrieval of relevant chunks of
    the data.

    Storing data locally and utilizing local search enables the retrieval of
    relevant portions of the data at lower costs. These chunks can be
    provided to the language model as context or examples within the prompt.
    Consequently, the language model can comprehend these examples and generate
    a more accurate and relevant response by leveraging this context.

    This is where LlamaIndex comes into play. Also sometimes referred to as GPT
    Index, LlamaIndex helps you connect Large Language Models (LLMs) with your
    data. LlamaIndex offers a comprehensive set of tools and best-practices
    specifically designed for efficient and cost-effective downloading,
    indexing, and querying of your data.

    ## Fundamental Concepts of LlamaIndex

    LlamaIndex is generally [used in the following manner](https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html):

    1. Data Loading: 
    First, we retrieve the data to be indexed and convert it to documents, which includes metadata.

    2. Data Processing: 
    Next, we parse the document into nodes, in other words, chunks of text.

    3. Data Indexing: 
    With the data loaded and processed, we index it to enable search and retrieval of nodes.

    4. Querying: 
    Finally, we can execute a query and receive a response. Behind the scenes, this query is run against the index to retrieve documents and nodes relevant to the query. An LLM is used to generate the final response.

    5. Optimization: 
    If needed, LlamaIndex also be used to improve the quality of responses and reduce usage costs.

    ## Data Loading

    LlamaIndex's data connectors, also known as loaders, streamline the process
    of loading data from online sources, such as Google Docs and Slack, and
    offline sources, such as text files and PDF files.

    Most often, the downloaded data is indexed and loaded into a vector store.
    This vector store can then be queried to retrieve relevant portions, or
    ”chunks”, of the data. These chunks can subsequently be included as
    examples in the prompt.

    A list of LlamaIndex loaders can be
    found online at [llamahub.ai](https://llamahub.ai/).

    ## Data Processing

    Data processing involves, for example, splitting the loaded [documents into
    nodes](https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/SentenceSplittingDemo.ipynb) and [extracting
    keywords](https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/KeywordTableComparison.ipynb).

    ## Data Indexing

    Data indexing is a process in which raw textual data is transformed into a
    format that LLMs can efficiently understand and work with. During this
    process, the data is converted into vector representations, often
    referred to as embeddings.

    LlamaIndex not only provides tools for indexing data easily and
    efficiently, but also enables the creation of complex data structures, such
    as indices organized into graph or tree structures that are optimized for
    specific tasks or queries. Indices can be organized, for example, by
    keyword, relationships, etc. Furthermore, each index can have a textual
    description (summary) that informs the language model, such as GPT, about
    the context in which the index is most suitable.

    ## Data Structures

    LlamaIndex offers a variety of data structures that simplify the development
    and maintenance of LLM-based applications. These data structures allow
    efficient organization, retrieval, and querying of information for large
    language models. The basic data structures provided by LlamaIndex include:

    - Document

    The document holds the raw data, for example, text and metadata.

    - Node

    Nodes are chunks of the documents (e.g. sentences or paragraphs). Due to
    prompt size constraints—often referred to as 'context window size'—we are
    not allowed to feed large documents to the LLMs. One solution is to break
    the document down into smaller pieces, a process known as ”chunking”. For
    example, the GPT-4 model has a context window size of 32,000 tokens, with
    each token representing a character or word.

    - Index

    The indexing process transforms chunks of data or entire documents into
    numeric vector representations (embeddings) that enable the LLM to work
    with it.

    Indices are often stored in vector stores or databases, such as ChromaDB,
    Milvus, Pinecone, Qdrant, Redis, Weaviate or Zilliz.

    - Index structures

    At a minimum, a single index is required to query your data. However, you
    can also create multiple indices (lists, graphs, trees) for greater
    flexibility. Organizing indices into graph and tree structures can improve
    the organization and retrieval of relevant documents.

    - Storage context

    LlamaIndex enables the reuse of nodes between multiple indices, thus
    preventing duplication of data across indices. This feature allows for more
    efficient resource utilization and cost reduction.

    - Cost predictor

    LlamaIndex's cost predictors help estimate the expenses associated with
    using language models such as OpenAI's GPT models.

    Each interaction with a LLM incurs a cost - for example, OpenAI's Davinci
    costs $0.02 per 1,000 tokens at the time of writing. The cost associated
    with building an index and querying depends on many factors, including:

      - The LLM used, e.g., GPT-4 is more expensive than GPT-3
      - The size of the data, i.e., the number of tokens used during indexing
      - The size of the prompt, i.e., the number of tokens used during prompting

    - Prompt template

    LlamaIndex's prompt templates are optimized for building efficient prompts
    (queries). Prompt engineering is a crucial concept in working with LLMs, as
    it helps improve the quality of responses.

    In addition to prompts, there are techniques and concepts that are
    important to understand to be able to use LLMs and LlamaIndex efficiently.

    Prompt engineering is perhaps the most important concept to learn for the
    end-user of LLMs.

    ## Prompt Engineering

    Prompt engineering involves designing effective prompts (input) for Large
    Language Models (LLMs) to improve their responses (output). The quality
    of the input prompts is crucial to the effectiveness of an LLM.

    For instance, an effective prompt that guides the LLM towards a correct
    answer may include context, facts, and specify what to avoid or include in
    the response.

    LlamaIndex provides a range of [prompt
    templates](https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py)
    optimized for various tasks such as [question-answering, prompt refinement,
    keyword extraction, code generation, and
    more](https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py).

    Prompt engineering is an evolving field of research, with new techniques
    being continually developed.

    ## Model

    In addition to the the prompt, the choice of LLM model plays an important
    role in the response quality. Different models have distinct
    characteristics and strengths, which affect their performance and the
    quality of the responses they are able to generate.

    For example, OpenAI's text-davinci-003 model for ChatGPT is a
    cost-efficient option and serves as the default model employed by
    LlamaIndex. However, the optimal model depends on the use case. OpenAI's
    Codex model, for example, is optimized for generating and understanding code.

    With LlamaIndex you can choose a different, non-default, LLM model when
    constructing an index.

    Evaluating the performance of a model for your specific use case is a
    crucial aspect of developing applications that leverage language models.

    In addition to optimal use cases, each model has associated usage costs and
    varying computational resource requirements, such as memory and CPU.

    ## Vector Store

    LlamaIndex offers the option of using vector stores, also referred to as
    embeddings databases, to address high usage costs, prompt-size limitations,
    search quality issues, and many other challenges and limitations of LLM
    models and the limited data they have been are trained on.

    Vector stores function as local or remotely hosted databases that are used
    for storing and searching vector representations of textual data. These
    vector representations, known as embeddings in the context of LLMs,
    capture the semantic and syntactic meaning of the text.

    The primary purpose of vector stores is to enable fast and efficient
    retrieval of chunks of text (embeddings) relevant to the prompt from large
    datasets that the language model has not been trained on.

    For example, you can store a recently published and non-public PDF that the
    LLM has not been trained on and include chunks of the PDF in the prompt.

    Popular vector store implementations include:

    - ChromaDb: an open-source vector store that can be hosted locally
    - Pinecone: a closed-source vector store that is provided as a SaaS-service

    ## Similarity Search

    Similarity search algorithms can be used to find and retrieve relevant
    chunks of text that are relevant to the prompt. The text can then be
    incorporated as, for example, context (examples) within the prompt template
    to generate a final prompt that is sent to the LLM.

    Vector stores utilize various search algorithms, such as approximate
    nearest neighbor (ANN), which can efficiently store, locate, and retrieve
    text most similar to the user query.

    Some popular ANN libraries include FAISS, Annoy, and HNSW.

    LlamaIndex is also able to use these libraries without a vector store.

    ## LlamaIndex's Core Abstractions

    [LlamaIndex's three core abstractions](https://betterprogramming.pub/llamaindex-0-6-0-a-new-query-interface-over-your-data-331996d47e89) are:

    1. Index: Offers storage and query capabilities for documents stored as chunks of text (nodes), including metadata.
    2. Retriever: Retrieves the most relevant data (nodes) from an index based on a given query.
    3. QueryEngine: Generates a response based on the query and retrieved nodes.

    ![LlamaIndex core concepts](/images/articles/llama-core-concepts.png "LlamaIndex core concepts. Source https://gpt-index.readthedocs.io/")

    In addition to these core abstractions, LlamaIndex has numerous other
    concepts, some of which are discussed later in this article.

    Please note, this article is based on LlamaIndex version 0.6. Future
    versions might introduce new concepts or rename existing ones.

    ## Index & Index Composability

    An index provides an efficient view of your data, enabling quick and
    effective retrieval of information relevant to a prompt. In LlamaIndex, a
    retriever is used to identify and return the nodes that are most relevant
    to your query.

    When the corpus of data is large, using similarity search alone on one
    index may not be sufficient to find the most relevant data. This is because
    the result set will also be large in these cases.

    For complex use cases like these, where having one huge index leads to poor
    results, the concept of index composability offers a solution for better
    results. This approach, allows you to split the indices into smaller
    segments, consequently minimizing the number of results. Essentially, these
    structures allow us to break down the query process into separate parts
    making the search space smaller and more optimized.

    LlamaIndex's index-composability features enable creating a knowledge graph
    (e.g., list, tree, and table are supported) from a set of indices. The
    knowledge graph of indices can then be queried, allowing LlamaIndex
    (sometimes with the help of the LLM) to choose the indices that are most
    relevant to the query.

    When composing a hierarchy of indices, you provide LlamaIndex (and the LLM)
    with a summary of each index. The summary helps the LLM in selecting the
    best indices for your query.

    LlamaIndex uses query engines to execute the query recursively, traversing
    the indices, starting from the root index, then moving to the sub-indices.

    ## Retriever

    In order to retrieve your data (as nodes) from an index, LlamaIndex uses
    [retrievers](https://gpt-index.readthedocs.io/en/latest/reference/query/retrievers.html).

    LlamaIndex supports many types of retrievers, including the following:

    - Vector store retriever: returns top-k most similar nodes from a vector store
    - List retriever: capable of returning all nodes from an index
    - Tree retriever: capable of returning nodes from a hierarchical tree of nodes

    Each type of retriever is optimized to a specific use case.

    ## Query Engine

    A query engine processes a given query and generates a response by
    utilizing a retriever and a response synthesizer. This is the component you
    interact with when prompting an LLM through LlamaIndex.

    The query engine generates a response that includes the answer along with
    the sources of the answer (nodes).

    Behind the three core concepts LlamaIndex there are even more concepts,
    such as the response synthesizer discussed next.

    ## Response Synthesizer

    A [response
    synthesizer](https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html#response-synthesis)
    takes a list of nodes as input, which are relevant document chunks (nodes)
    fetched by a retriever from an index, and generates an output in the form
    of a response or answer to the given query.

    LlamaIndex supports different types of response synthsis, including:

    - Create-and-Refine: Iteratively approach of generating a response from a list of nodes.
    - Tree-Summarize: Bottoms-up approach of generating a response from a tree where the nodes are the leaves.

    ## Query Transformation

    LlamaIndex supports automatic prompt engineering techniques that aim to
    improve the prompt in order to obtain a better answer from the LLM:

    - HyDE (Hypothetical Document Embeddings)
    - Single-Step Query Decomposition
    - Multi-Step Query Transformations

    The [Hypothetical Document Embeddings (HyDE) query
    transform](https://arxiv.org/abs/2212.10496) improves accuracy by
    generating a hypothetical document as explained in the original paper,
    published by Luyu Gao in 2022:

      > While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document.

      > Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages~(e.g. sw, ko, ja)

    ## Post Processors

    The LlamaIndex post-processor functionality enables filtering and
    augmentation of search results (nodes) before sending it to the response
    synthesizer. For instance, you can exclude or require specific keywords to
    be absent or present in the retrieved nodes.

    Furthermore, you can rank results based on attributes such as time or other
    characteristics.

    The query engine handles post-processing of nodes retrieved from the index.

    ## Response Evaluation

    The quality of the response can be evaluated using tools provided by
    LlamaIndex. This allows us to choose the most suitable language model and
    configuration for our problem, and minimize the number of errors and
    hallucinations.

    In LlamaIndex, The Playground module offers an automated method for testing
    your data across a wide range of combinations involving indices, models,
    embeddings, modes, and more.

    This helps you determine the optimal configuration for your specific needs.

    ## Cost Reduction

    Cost optimization and reduction are important considerations when building
    LLM-based applications.

    For instance, ChatGPT charges by usage (e.g., tokens), so it makes sense to
    try to minimize the number of tokens used. With LlamaIndex's optimizer
    features, such as the Token Predictor, you can optimize costs by predicting
    and minimizing the number of tokens sent to ChatGPT.

    Costs can also be optimized through configuration options, such as, the
    selection of the model. The Playground module gives you a way of
    automatically testing your data using different configurations of indices,
    models, and many other options.

    ## Conclusion

    LlamaIndex offers a comprehensive toolset for processing and indexing your
    data, as well as optimizing the performance, quality, and cost-efficiency
    of your Large Language Model-based applications.

    While implementing your own toolset is possible, understanding the concepts
    and techniques underlying LlamaIndex is beneficial, especially if you are
    new to the field of Large Language Models (LLMs).
