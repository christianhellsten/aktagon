extends ../article-layout.pug

block title
  title= 'Index and Search Your Data Seamlessly with LlamaIndex and Large Language Models'

block content
  h1 Index and Search Your Data Seamlessly with LlamaIndex and Large Language Models

  :markdown-it(linkify langPrefix='highlight-' plugins=['markdown-it-table-of-contents', 'markdown-it-anchor'])

    **Table of contents**

    [[toc]]

    ## Introduction

    Large language models (LLMs) like ChatGPT have a human-like ability to
    understand languages, enabling them to respond to questions and prompts in
    a manner that resembles a human conversation.

    Despite this impressive ability, there is always a risk that an LLM will
    fail to provide a relevant and accurate answer. This can be attributed to
    various factors, primarily the limited, and sometimes the outdated, nature
    of the data on which the LLM was trained on, but also the stochastic nature
    of the algorithms they employ.

    As an example, ChatGPT will not be able to provide relevant answers when
    the question concerns private data that the language model has not been
    trained on.

    In such cases, LLMs can be fine-tuned using the private data to achieve
    more accurate responses. However, fine-tuning can be a resource intensive
    and costly operation, and has to be done again whenever the data is
    updated. Fine-tuning might also cause problems such as overfitting.

    Often a more effective and cost-efficient approach involves prompt
    engineering and storing data locally before sending it to the LLM.

    Storing data locally and utilizing similarity search enables the retrieval
    of relevant portions of the data at lower costs. These ”chunks” can be
    provided to the language model as context, referred to as context-augmented
    generation, within the prompt.

    The language model can comprehend these examples and generate
    a more accurate and relevant response by leveraging this context.

    This is where LlamaIndex comes into play. Also sometimes referred to as GPT
    Index, LlamaIndex helps you connect Large Language Models (LLMs) with your
    data. LlamaIndex offers a comprehensive set of tools and best-practices
    specifically designed for efficient and cost-effective downloading,
    indexing, and querying of your data.

    This article is based on LlamaIndex version 0.6.

    ## LlamaIndex Workflow

    As a software engineer, this is [how I most often use
    LlamaIndex](https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html):

    **Data Loading**

    First, I retrieve the data to be indexed and convert it to documents using
    LlamaIndex.

    **Data Processing**

    Next, I process the data. This includes parsing the document into nodes, in
    other words, chunks of text, extracting keywords, etc.

    **Data Indexing**

    With the data loaded and processed, I use LlamaIndex to index the data.
    This enables search and retrieval of nodes that are relevant to the query.

    **Querying**

    The last step is to execute a query. Behind the scenes, the query is run
    against the index to retrieve documents and nodes relevant to the query. An
    LLM model is used to generate the final response.

    **Optimization**

    LlamaIndex can optionally be used to optimize the quality of responses and
    reduce usage costs.

    ## Core Concepts

    LlamaIndex offers a variety of features and data structures that simplify
    the development and maintenance of LLM-based applications. These features
    allow efficient organization and querying of information for large language
    models. The core features provided by LlamaIndex include:

    ![LlamaIndex core concepts](/images/articles/LlamaIndex.jpg "LlamaIndex core concepts")

    **Index**

    An index allows for efficient queries, most often some type of similarity
    search, to be run on your data. The indexing process transforms chunks of
    data or entire documents into numerical vector representations.

    **Retriever**

    A retriever is used to retrieve nodes from an index.

    **Response synthesizer**

    The response synthesizer generates the final response. Response synthesis
    can optionally include post processors and various optimizers to improve
    response quality and reduce costs.

    **Query engine**

    The query engine serves as the interface and acts as a bridge between our
    data and the LLM. It allows us to pose questions through prompts and
    receive answers from the LLM.

    ## Data Structures

    Behind LlamaIndex, there is a collection of data structures that enable its
    functionality, these are some of the more important ones:

    **Data loader**

    LlamaIndex's data connectors, also known as loaders, streamline the process
    of loading data from online sources, such as Google Docs and Slack, and
    offline sources, such as text files and PDF files.

    A list of LlamaIndex loaders can be
    found online at [llamahub.ai](https://llamahub.ai/).

    **Document**

    Documents hold the raw data, for example, the text and metadata of one or more pages of a PDF file.

    **Node**

    Nodes are chunks of the documents such as words, sentences, and paragraphs.
    Due to prompt size constraints, often referred to as ”context window size”,
    it's not possible to feed large documents to the LLMs. One solution is to
    break the document down into smaller pieces, a process known as ”chunking”.
    The GPT-4 model, for example, has a context window size of 32,000 tokens,
    with each token representing a character or word.

    **Token**

    A token is the base unit, a character, word, or sentence, that the LLM
    model is able to work with.

    A tokenizer is responsible for splitting the text into tokens.

    **Embeddings**

    In the field of natural language processing, an embedding refers to a
    vector, which can be thought of as a list of numbers. This vector
    representation of a word or a sequence of words captures its meaning and
    grammar in a way that enables the model to process and comprehend the text.

    **Prompts**

    Interaction with an LLM model is done through a prompt. This is why writing
    efficient prompts, often referred to as Prompt Engineering, is the key to
    getting the most out of LLM models. LlamaIndex includes prompt templates that
    follow best practices and also allows you to customize them to your needs.

    ## Prompt Engineering

    The quality of the prompts is crucial to the effectiveness of an LLM.

    Prompt engineering involves designing effective prompts (input) for Large
    Language Models (LLMs) to improve their responses (output).

    For instance, an effective prompt that guides the LLM towards a correct
    answer may include context, facts, and specify what to avoid or include in
    the response.

    LlamaIndex provides a range of [prompt
    templates](https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py)
    optimized for various tasks such as [question-answering, prompt refinement,
    keyword extraction, code generation, and
    more](https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py).

    Prompt engineering is an evolving field of research, with new techniques
    being continually developed.

    ## Model

    In addition to the the prompt, the choice of LLM model plays an important
    role in the response quality. Different models have distinct
    characteristics and strengths, which affect their performance and the
    quality of the responses they are able to generate.

    For example, OpenAI's text-davinci-003 model for ChatGPT is a
    cost-efficient option and serves as the default model employed by
    LlamaIndex. However, the optimal model depends on the use case. OpenAI's
    Codex model, for example, is optimized for generating and understanding code.

    With LlamaIndex you can choose a different, non-default, LLM model when
    constructing an index.

    Evaluating the performance of a model for your specific use case is a
    crucial aspect of developing applications that leverage language models.

    ## Vector Store

    Vectors, referred to as embeddings in the context of Large Language Models,
    are numerical representations of the nodes and tokens that Large Language
    Models are able to understand and process. These are not features of
    LlamaIndex per se, but fundamental concepts used in machine learning and
    the development of LLM models.

    Vectors, along with metadata, are most often stored in-memory or in a
    vector database.

    Vector stores enable fast and efficient retrieval of text (embeddings)
    relevant to the prompt from large datasets that the language model has not
    been trained on.

    They achieve this through the following key features, which are essential in
    the development of LLM-based applications:

    - **Vector storage**: Vector stores provide a mechanism to store and organize vectors (embeddings) efficiently.
    - **Indexing capabilities**: Vector stores offer indexing capabilities, which makes vector searching and retrieval faster.
    - **Similarity search**: Vector stores enable similarity search, which involves finding vectors that are most similar to a given query vector.

    LlamaIndex provides supports many popular vector stores, including, Milvus,
    Redis, Pinecone, etc.

    ## Similarity Search

    Vector stores use various search algorithms, such as ANN (Approximate
    Nearest Neighbors), to find text that is similar to the query prompt.

    These text chunks are then incorporated as context within the prompt. By
    doing so, the LLM can use the provided context to generate a more informed
    and accurate response to the prompt.

    LlamaIndex supports several ANN libraries, including FAISS, Annoy, and
    HNSW. These libraries enable efficient and accurate approximate nearest
    neighbor search operations.

    ## Index & Index Composability

    LlamaIndex offers many types of indices, each designed to store nodes in
    distinct ways and execute data queries using different approaches.

    When the corpus of data is large, using similarity search alone on one
    index may not be sufficient to find the most relevant data. This is because
    the result set will also be large in these cases.

    For complex use cases like these, where having one huge index leads to poor
    results, the concept of index composability offers a solution for better
    results

    Software engineers can then choose the most suitable index type(s) for their
    problem and build a _composable graph_ from multiple indices.

    **List index**

    A list-based data structure that takes documents as input and breaks them
    up into small document chunks.

    Uses the _create and refine_ paradigm to construct an initial answer using
    the first text chunk.

    The answer undergoes further refinement by using subsequent text chunks as
    context. This iterative process of feeding in additional information can
    enhance the accuracy and depth of the answer, allowing for a more
    comprehensive and context-aware response.

    Does not use an LLM during index construction.

    **Vector store index**

    A vector store index is built on top of a vector store.

    Uses the LLM model to create the embeddings during index construction.

    **Tree index**

    The tree index is _a tree-structured index_ in which each node serves as a
    summary of its corresponding children nodes. This tree-based organization
    allows for efficient navigation and retrieval of information, with each
    parent node encapsulating a concise representation of its child nodes'
    content.

    During querying the tree of indices is traversed to construct an answer.

    A tree index is useful for summarizing a collection of documents.

    **Composable graph index**

    In a [_composable graph index_](https://howaibuildthis.substack.com/p/llamaindex-how-to-use-index-correctly), you can specify one index as the root and specify
    separate children indices. Each child index should have a summary.

    During querying the graph of indices is traversed recursively to construct
    an answer.

    **Knowledge graph index**

    Uses GPT with a _keyword extraction prompt_ to extract keywords from
    document chunks during index creation.

    During querying, extracts a set of relevant keywords from the query using a
    keyword extraction prompt. The retrieved chunks are then ranked based on
    the number of matches with the keywords, ensuring the most relevant results
    are prioritized.

    Constructs the answer using the _create and refine_ paradigm.

    Similar to a hash table. Also referred to as keyword-table index.

    **SQL index**

    The SQL index allows you to connect your database data to an LLM and ask questions about the data.

    **Pandas index**

    The Pandas index allows you to connect pandas to an LLM and ask questions about the data.

    ## Retriever

    In order to retrieve your data (as nodes) from an index, LlamaIndex uses
    [retrievers](https://gpt-index.readthedocs.io/en/latest/reference/query/retrievers.html).

    LlamaIndex supports many types of retrievers, including the following:

    - Vector store retriever: returns top-k most similar nodes from a vector store.
    - List retriever: capable of returning all nodes from an index.
    - Tree retriever: capable of returning nodes from a hierarchical tree of nodes.
    - Knowledge graph retriever: capable of returning nodes from a hierarchical tree of nodes.

    ## Query Engine

    A query engine processes a given query and generates a response by
    utilizing a retriever and a response synthesizer. This is the component you
    interact with when prompting an LLM through LlamaIndex.

    The query engine generates a response that includes the answer along with
    the sources of the answer (nodes).

    ## Response Synthesizer

    A [response
    synthesizer](https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html#response-synthesis)
    takes a list of nodes as input, which are relevant document chunks (nodes)
    fetched by a retriever from an index, and generates an output in the form
    of a response or answer to the given query.

    LlamaIndex supports different types of response synthsis, including:

    - Create-and-Refine: Iteratively approach of generating a response from a list of nodes.
    - Tree-Summarize: Bottoms-up approach of generating a response from a tree where the nodes are the leaves.

    ## Query Transformation

    LlamaIndex supports automatic prompt engineering techniques that aim to
    improve the prompt in order to obtain a better answer from the LLM:

    - HyDE (Hypothetical Document Embeddings)
    - Single-Step Query Decomposition
    - Multi-Step Query Transformations

    The [Hypothetical Document Embeddings (HyDE) query
    transform](https://arxiv.org/abs/2212.10496) improves accuracy by
    generating a hypothetical document as explained in the original paper,
    published by Luyu Gao in 2022:

      > While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document.

      > Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages~(e.g. sw, ko, ja)

    ## Post Processors

    The LlamaIndex post-processor functionality enables filtering and
    augmentation of search results (nodes) before sending it to the response
    synthesizer. For instance, you can exclude or require specific keywords to
    be absent or present in the retrieved nodes.

    Furthermore, you can rank results based on attributes such as time or other
    characteristics.

    The query engine handles post-processing of nodes retrieved from the index.

    ## Response Evaluation

    The quality of the response can be evaluated using tools provided by
    LlamaIndex. This allows us to choose the most suitable language model and
    configuration for our problem, and minimize the number of errors and
    hallucinations.

    In LlamaIndex, The Playground module offers an automated method for testing
    your data across a wide range of combinations involving indices, models,
    embeddings, modes, and more.

    This helps you determine the optimal configuration for your specific needs.

    ## Cost Reduction

    Cost optimization and reduction are important considerations when building
    LLM-based applications.

    For instance, ChatGPT charges by usage (e.g., tokens), so it makes sense to
    try to minimize the number of tokens used. With LlamaIndex's optimizer
    features, such as the Token Predictor, you can optimize costs by predicting
    and minimizing the number of tokens sent to ChatGPT.

    Costs can also be optimized through configuration options, such as, the
    selection of the model. The Playground module gives you a way of
    automatically testing your data using different configurations of indices,
    models, and many other options.

    **Cost predictor**

    LlamaIndex's cost predictors help estimate the expenses associated with
    using language models such as OpenAI's GPT models.

    Each interaction with a LLM model incurs a cost - for example, OpenAI's
    Davinci costs $0.02 per 1,000 tokens at the time of writing (2023). The
    cost associated with building an index and querying depends on many
    factors, including:

      - The LLM used, e.g., GPT-4 is more expensive than GPT-3
      - The size of the data, i.e., the number of tokens used during indexing
      - The size of the prompt, i.e., the number of tokens used during prompting

    **Storage context**

    LlamaIndex storage context features allows for the reuse of nodes between
    multiple indices, thus lowering costs by preventing duplication of data
    across indices.

    ## Conclusion

    LlamaIndex offers a comprehensive toolset for processing and indexing your
    data, as well as optimizing the performance, quality, and cost-efficiency
    of your Large Language Model-based applications.

    While implementing your own toolset is possible, understanding the concepts
    and techniques underlying LlamaIndex is beneficial, especially if you are
    new to the field of Large Language Models (LLMs).
